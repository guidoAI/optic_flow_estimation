{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optic flow estimation from an image pair\n",
    "\n",
    "In this notebook, we will explain how to estimate optic flow from an image pair coming from a conventional camera. There are many different optic flow algorithms, ranging from exhaustive block-matching to variational methods, and from phase-based methods to deep neural networks. However, in our explanation, we will focus on the most common, CPU-based, _feature-based method_ that performs two steps:\n",
    "<OL>\n",
    "    <LI> Feature detection </LI>\n",
    "    <LI> Feature tracking </LI>\n",
    "</OL>\n",
    "\n",
    "In this notebook, we will start with the second step, as it is the one that actually determines the optic flow. In particular, we will explain the method developed by Lucas and Kanade [1], which is both a fast and accurate method for determining optic flow. After that we will explain the Harris corner detector [2]. \n",
    "\n",
    "## Lucas and Kanade's algorithm for optic flow determination\n",
    "In their article, Lucas and Kanade first explain their method with a one-dimensional example. We will do the same here. Below, we show two curves, $f(x)$ and $g(x)$, which only differ by a horizontal shift, i.e., $g(x)=f(x+h)$, where $h$ is the the horizontal shift. The vertical axis is termed $I$ for illuminance. The fact that the curves have the exact same illuminance shows that it constant brightness is assumed. \n",
    "\n",
    "<IMG SRC=\"LucasKanade_flow.png\" WIDTH=\"50%\"></IMG>\n",
    "\n",
    "So, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "g(x) = f(x+h)\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 1)</div>\n",
    "\n",
    "The basis for the Lucas and Kanade method is a linear approximation:\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x)}{h}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 2)</div>\n",
    "\n",
    "where we can use equation 1 to arrive at:\n",
    "\n",
    "\\begin{equation}\n",
    "f'(x) \\approx \\frac{g(x) - f(x)}{h}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 3)</div>\n",
    "\n",
    "and by re-arranging terms:\n",
    "\n",
    "\\begin{equation}\n",
    "h \\approx \\frac{g(x) - f(x)}{f'(x)}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 4)</div>\n",
    "\n",
    "Eq. 4 is the basic step in the Lucas and Kanade algorithm. Explained on an intuitive level: If function $g$ is higher at $x$ than $f$, and $f'(x)$ is positive, then $h$ should be augmented in order to shift $f$ further to the left to better match $g$. Similar reasonings can be follow for $g$ being lower than $f$ and $f'(x)$ being positive or negative. Both the underlying math and the intuitive explanation immediately show that this method strongly relies on local approximations.\n",
    "\n",
    "<font color='red'><B>Exercise 1.</B></font> You are now going to explore the characteristics of the very elementary form of the LK method as presented in Eq. 4. \n",
    "\n",
    "1. Please have a look at the code. Halfway the code, you will find a variable `x_query`, which is now set to 3. If you run the code, you will see that the optic flow estimated at that location corresponds quite well to the ground truth value of 0.5. Set `x_query` to different values. Try to make the algorithm fail. Under what conditions does it fail?\n",
    "\n",
    "2. Now change the function properties. Can you make a function for which the method will always fail? And a function for which the estimate will always be wrong when the shift is bigger than a given threshold? What do these situations correspond to, if the function values correspond to actual pixel values in a real image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def fun(x, offset=10, slope = -0.5, a = 1, fun_type = 'sine_slope'):    \n",
    "    \n",
    "    if(fun_type == 'sine_slope'):\n",
    "        y = offset + slope * x + np.sin(a * x)\n",
    "    elif(fun_type == 'quadratic'):\n",
    "        y = offset + slope * x + a * x**2\n",
    "    elif(fun_type == 'linear'):\n",
    "        y = offset + slope * x\n",
    "        \n",
    "    return y\n",
    "\n",
    "# ground-truth shift:\n",
    "dx = 0.5\n",
    "\n",
    "# properties of the function\n",
    "fun_type = 'quadratic'\n",
    "offset= 10 #10\n",
    "slope = 0 #-0.5\n",
    "a = 2\n",
    "\n",
    "# range in which we show the function:\n",
    "max_x = 3\n",
    "step = 0.01 \n",
    "\n",
    "# We take a simple 1D function and shift it in the x-axis:\n",
    "x = np.arange(-max_x, max_x, step)\n",
    "f = fun(x, offset, slope, a, fun_type)\n",
    "g = fun(x+dx, offset, slope, a, fun_type)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure()\n",
    "plt.plot(x, f, x, g)\n",
    "plt.xticks(np.arange(-max_x, max_x,1))\n",
    "plt.grid()\n",
    "plt.legend(['f', 'g'])\n",
    "plt.show()\n",
    "\n",
    "# x coordinate at which we apply Eq. 4:\n",
    "x_query = 2\n",
    "\n",
    "# Determine f'(x) and apply Eq. 4:\n",
    "small = 0.01\n",
    "df_dx = (fun(x_query+small, offset, slope, a, fun_type) - fun(x_query, offset, slope, a, fun_type)) / small\n",
    "h = (fun(x_query+dx, offset, slope, a, fun_type) - fun(x_query, offset, slope, a, fun_type)) / df_dx\n",
    "\n",
    "# Output result:\n",
    "print(f'Lucas and Kanade say that the shift at {x_query} = {h}, while the ground truth is {dx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wider support for determining optic flow\n",
    "\n",
    "Normally the Lucas Kanade method determines optic flow by evaluating not a single pixel but a window. A naive way of using multiple pixels would be to average the proposed shifts:\n",
    "\n",
    "\\begin{equation}\n",
    "h \\leftarrow \\frac{\\sum_{x \\in W} \\frac{g(x) - f(x)}{f'(x)}}{\\sum_{x \\in W} 1}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 5)</div>\n",
    "\n",
    "In [1], it is remarked that the approximation in Eq. 2 is most accurate if $f''(x)$ is small:\n",
    "\n",
    "\\begin{equation}\n",
    "f''(x) \\approx \\frac{g'(x) - f'(x)}{h}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 6)</div>\n",
    "\n",
    "Hence, it is proposed to use the following weight (since $h$ is the same for each pixel, it is omitted):\n",
    "\n",
    "\\begin{equation}\n",
    "w(x) = \\frac{1}{|g'(x) - f'(x)|}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 7)</div>\n",
    "\n",
    "Leading to a weighted estimate:\n",
    "\n",
    "\\begin{equation}\n",
    "h \\leftarrow \\frac{\\sum_{x \\in W} w(x) \\frac{\\left( g(x) - f(x) \\right)}{f'(x)}}{\\sum_{x \\in W} w(x)}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 8)</div>\n",
    "\n",
    "<font color='red'><B>Exercise 2.</B></font>\n",
    "1. We have implemented both weighing schemes (Eqq. 5, 8). By changing the query coordinate, try to find out which scheme works better. Are there some coordinates where the weighted schemes still work badly? Try out both the quadratic and sine slope functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth shift:\n",
    "dx = 0.5\n",
    "\n",
    "# properties of the function\n",
    "fun_type = 'quadratic'\n",
    "offset= 10 #10\n",
    "slope = 0 #-0.5\n",
    "a = 2\n",
    "\n",
    "#fun_type = 'sine_slope'\n",
    "#offset= 5\n",
    "#slope = 0.5\n",
    "#a = 1\n",
    "\n",
    "# range in which we show the function:\n",
    "max_x = 3\n",
    "step = 0.01 \n",
    "\n",
    "# properties of the window:\n",
    "window_half_size = 1\n",
    "window_size = 2 * window_half_size + 1\n",
    "window_step = 0.25\n",
    "\n",
    "# We take a simple 1D function and shift it in the x-axis:\n",
    "x = np.arange(-max_x, max_x, step)\n",
    "f = fun(x, offset, slope, a, fun_type)\n",
    "g = fun(x+dx, offset, slope, a, fun_type)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure()\n",
    "plt.plot(x, f, x, g)\n",
    "plt.xticks(np.arange(-max_x, max_x,1))\n",
    "plt.grid()\n",
    "plt.legend(['f', 'g'])\n",
    "plt.show()\n",
    "\n",
    "# x coordinate at which we apply Lucas Kanade:\n",
    "x_query = 2\n",
    "\n",
    "estimates = np.zeros([window_size, 1])\n",
    "weights = np.zeros([window_size, 1])\n",
    "\n",
    "small = 0.01\n",
    "i = 0\n",
    "for st in np.arange(x_query-window_step, x_query+1.5*window_step, window_step):\n",
    "    df_dx = (fun(st+small, offset, slope, a, fun_type) - fun(st, offset, slope, a, fun_type)) / small\n",
    "    h = (fun(st+dx, offset, slope, a, fun_type) - fun(st, offset, slope, a, fun_type)) / df_dx\n",
    "    estimates[i] = h\n",
    "    print(f'Estimate at {st} = {h}')\n",
    "    \n",
    "    dg_dx = (fun(st+dx+small, offset, slope, a, fun_type) - fun(st+dx, offset, slope, a, fun_type)) / small\n",
    "    nom = np.abs(dg_dx - df_dx)\n",
    "    print(f'df_dx = {df_dx}, dg_dx = {dg_dx}, nominator = {nom}')\n",
    "    if(nom > 1E-3):\n",
    "        weights[i] = 1 / nom\n",
    "    else:\n",
    "        weights[i] = 1E3\n",
    "    print(f'Weight at {st} = {weights[i]}')\n",
    "    i += 1\n",
    "\n",
    "h_Eq5 = np.mean(estimates)\n",
    "h_Eq8 = np.dot(np.transpose(weights), estimates) / np.sum(weights) \n",
    "h_Eq8 = h_Eq8[0][0]\n",
    "\n",
    "# Output result:\n",
    "print(f'Averaging the windows estimates at {x_query} leads to {h_Eq5}, while the ground truth is {dx}')\n",
    "print(f'Weighing the windows estimates at {x_query} leads to {h_Eq8}, while the ground truth is {dx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformulation\n",
    "Lucas and Kanade subsequently reformulate the estimation, to prevent division by zero and ensure generalization to two dimensions. They start with:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x+h) \\approx f(x) + h f'(x)\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 9)</div>\n",
    "\n",
    "And then want to minimize the quadratic error:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\sum_x \\left( f(x+h) - g(x) \\right)^2\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 10)</div>\n",
    "\n",
    "This can be done by setting its derivative to zero:\n",
    "\n",
    "\\begin{equation}\n",
    "0 = \\frac{\\delta E}{ \\delta h} = \\frac{\\delta}{ \\delta h} \\sum_x \\left( f(x) + h f'(x) - g(x) \\right)^2 = \\sum_x 2 f'(x) \\left( f(x) + h  f'(x) - g(x) \\right) \n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 11)</div>\n",
    "\n",
    "From which we can get:\n",
    "\n",
    "\\begin{equation}\n",
    "h = \\frac{\\sum_x f'(x) \\left( g(x) - f(x) \\right) }{\\sum_x f'(x)^2}\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 12)</div>\n",
    "\n",
    "This reformulation leads to a different weighting, with $w(x) = f'(x)^2$.\n",
    "\n",
    "<font color='red'><B>Exercise 3.</B></font>\n",
    "1. Does the new weighing in Eq. 12 solve the problem when $f'(x) = 0$? Try it out below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth shift:\n",
    "dx = 0.5\n",
    "\n",
    "# properties of the function\n",
    "fun_type = 'quadratic'\n",
    "offset= 10 #10\n",
    "slope = 0 #-0.5\n",
    "a = 2\n",
    "\n",
    "#fun_type = 'sine_slope'\n",
    "#offset= 5\n",
    "#slope = 0.5\n",
    "#a = 1\n",
    "\n",
    "# range in which we show the function:\n",
    "max_x = 3\n",
    "step = 0.01 \n",
    "\n",
    "# properties of the window:\n",
    "window_half_size = 1\n",
    "window_size = 2 * window_half_size + 1\n",
    "window_step = 0.25\n",
    "\n",
    "# We take a simple 1D function and shift it in the x-axis:\n",
    "x = np.arange(-max_x, max_x, step)\n",
    "f = fun(x, offset, slope, a, fun_type)\n",
    "g = fun(x+dx, offset, slope, a, fun_type)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure()\n",
    "plt.plot(x, f, x, g)\n",
    "plt.xticks(np.arange(-max_x, max_x,1))\n",
    "plt.grid()\n",
    "plt.legend(['f', 'g'])\n",
    "plt.show()\n",
    "\n",
    "# x coordinate at which we apply Lucas Kanade:\n",
    "x_query = 2\n",
    "\n",
    "estimates = np.zeros([window_size, 1])\n",
    "weights = np.zeros([window_size, 1])\n",
    "estimates_dfx = np.zeros([window_size, 1])\n",
    "weights_dfx = np.zeros([window_size, 1])\n",
    "\n",
    "small = 0.01\n",
    "i = 0\n",
    "for st in np.arange(x_query-window_step, x_query+1.5*window_step, window_step):\n",
    "    df_dx = (fun(st+small, offset, slope, a, fun_type) - fun(st, offset, slope, a, fun_type)) / small\n",
    "    h = (fun(st+dx, offset, slope, a, fun_type) - fun(st, offset, slope, a, fun_type)) / df_dx\n",
    "    estimates[i] = h\n",
    "    print(f'Estimate at {st} = {h}')\n",
    "    \n",
    "    dg_dx = (fun(st+dx+small, offset, slope, a, fun_type) - fun(st+dx, offset, slope, a, fun_type)) / small\n",
    "    nom = np.abs(dg_dx - df_dx)\n",
    "    print(f'df_dx = {df_dx}, dg_dx = {dg_dx}, nominator = {nom}')\n",
    "    if(nom > 1E-3):\n",
    "        weights[i] = 1 / nom\n",
    "    else:\n",
    "        weights[i] = 1E3\n",
    "    print(f'Weight at {st} = {weights[i]}')\n",
    "    \n",
    "    weights_dfx[i] = df_dx*df_dx\n",
    "    estimates_dfx[i] = df_dx*(fun(st+dx, offset, slope, a, fun_type) - fun(st, offset, slope, a, fun_type))\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "h_Eq5 = np.mean(estimates)\n",
    "h_Eq8 = np.dot(np.transpose(weights), estimates) / np.sum(weights) \n",
    "h_Eq8 = h_Eq8[0][0]\n",
    "h_Eq12 = np.sum(estimates_dfx) / np.sum(weights_dfx)\n",
    "\n",
    "# Output result:\n",
    "print(f'Averaging the windows estimates at {x_query} leads to {h_Eq5}, while the ground truth is {dx}')\n",
    "print(f'Weighing the windows estimates at {x_query} leads to {h_Eq8}, while the ground truth is {dx}')\n",
    "print(f'Reformulated weighing of the windows estimates at {x_query} leads to {h_Eq12}, while the ground truth is {dx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris corner detection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Harris corner detector', introduced by Harris and Stephens in [2], is one of the earliest and still well-performing corner detectors to date. It bases itself on an idea presented by Moravec [3], which identified corners by shifting a window around and then evaluating how much the image intensities in the window changed. A uniform image area would lead to no change and an edge only when shifting orthogonally to it. A corner would lead to the largest intensity changes when shifting the window around. Harris and Stephens adapted this idea in a way that it: (1) could be evaluated in place, (2) could better make the difference between corners and edges, and (3) was less noisy. \n",
    "\n",
    "The amount of intensity change when shifting by $(u,v)$ at locations $(x,y)$ in a window can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\sum_{(x,y) \\in W} w_{x,y} \\left( I(x+u, y+v) - I(x,y) \\right)^2\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 13)</div>\n",
    " \n",
    "Harris and Stephens approximate $I(x+u, y+v)$ with the Taylor expansion:\n",
    "\n",
    "\\begin{equation}\n",
    "I(x+u, y+v) = I(x,y) + u I_x(x,y) + v I_y(x,y) + O(x^2, y^2)\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 14)</div>\n",
    "\n",
    "Where $I_x(x,y) = \\frac{\\partial I(x,y)}{ \\partial x}$ is the horizontal image gradient and $I_y(x,y)$ is the vertical image gradient. When ignoring the second and higher order terms, this leads to:\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\sum_{(x,y) \\in W} w_{x,y} \\left( u I_x(x,y) + v I_y(x,y) \\right)^2\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 15)</div>\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\sum_{(x,y) \\in W} w_{x,y} \\left( u^2 I_x(x,y)^2 +  u v I_x(x,y) I_y(x,y) + v^2 I_y(x,y)^2 \\right)\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 16)</div>\n",
    "\n",
    "\\begin{equation}\n",
    "E(x_c,y_c) = (\\sum_W \\mathbf{w} \\odot I_x(W)^2) u^2 +  (\\sum_W \\mathbf{w} \\odot I_x(W) \\odot I_y(W)) u v +  (\\sum_W \\mathbf{w} \\odot I_y(W)^2) v^2 \\\\ = A u^2 + B uv + C v^2\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 17)</div>\n",
    "\n",
    "With $I_x(W)$ the horizontal gradients in the window $W$ centered at $(x_c,y_c)$, $\\odot$ the element-wise multiplication. Eq. 17 can also be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "E(x_c,y_c) = (u,v) M (u,v)^T\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 18)</div>\n",
    "\n",
    "With $M = \\begin{bmatrix} A && C \\\\ C && B \\end{bmatrix}$. Harris and Stephens remark that $E$ is closely related to the autocorrelation function. The eigenvalues of the matrix $M$, $\\lambda_1$ and $\\lambda_2$ will be proportional to the local autocorrelation function's curvature. Importantly, the eigenvalues do not depend on the exact rotation, so they form a rotation-invariant description. When both eigenvalues are small, the image window has little texture. When one eigenvalue is much larger than the other, the window contains an edge. When both eigenvalues are large, the window contains a corner. \n",
    "\n",
    "Finally, the Harris corner response value is based on the trace and determinant of the matrix $M$:\n",
    "\n",
    "\\begin{equation}\n",
    "R = \\mathrm{det}(M) - k \\mathrm{tr}(M)^2\n",
    "\\end{equation}\n",
    "<div style=\"text-align: right\">(Eq. 18)</div>\n",
    "\n",
    "Where $\\mathrm{det}(M) = \\lambda_1 \\lambda_2$ is the determinant, equal to the product of the eigenvalues, and $\\mathrm{tr}(M) = \\lambda_1 + \\lambda_2$ is the trace, equal to the sum of the eigenvalues. \n",
    "\n",
    "When $R < 0$, the image window contains an edge, when $R > 0$ it contains a corner, and when $R = 0$ the window contains uniform texture.\n",
    "\n",
    "<font color='red'><B>Exercise 4.</B></font>\n",
    "You will now have a look at an implementation of the Harris corner detector, based on the original article [2] and a blog by Muthukrishnan [4].\n",
    "\n",
    "1. Study the code below and run it. Unfortunately, the code still contains an error. Find and correct it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "\n",
    "def gradient_x(imggray):\n",
    "    # Sobel kernel for Ix.\n",
    "    kernel_x = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\n",
    "    return signal.convolve2d(imggray, kernel_x, mode='same')\n",
    "\n",
    "def gradient_y(imggray):\n",
    "    # Sobel kernel for Iy:\n",
    "    kernel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    return signal.convolve2d(imggray, kernel_y, mode='same')\n",
    "\n",
    "def non_maximum_suppression(Harris, dist = 1, threshold_factor = 0.05):\n",
    "    # suppress non maxima:\n",
    "    max_H = np.max(Harris[:])\n",
    "    threshold = threshold_factor * max_H\n",
    "    \n",
    "    Corners = np.zeros([Harris.shape[0], Harris.shape[1]])\n",
    "    for x in range(1, Harris.shape[1]-1):\n",
    "        for y in range(1, Harris.shape[0]-1):\n",
    "            if Harris[y,x] >= threshold:\n",
    "                Harris[y-dist:y+dist+1, x-dist:x+dist+1] = 0 # suppress neighbors\n",
    "                Corners[y,x] = 1 # indicate corner\n",
    "    return Corners\n",
    "\n",
    "def get_Harris_vectorized(filename, k = 0.04):\n",
    "    # load the BGR color image:\n",
    "    BGR = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(BGR, cv2.COLOR_BGR2GRAY)\n",
    "    height = gray.shape[0]\n",
    "    width = gray.shape[1]\n",
    "    \n",
    "    # resize if too big:\n",
    "    if(height * width >= 1E5):\n",
    "        gray = cv2.resize(gray, (int(width/4), int(height/4)))\n",
    "        BGR = cv2.resize(BGR, (int(width/4), int(height/4)))\n",
    "        height = gray.shape[0]\n",
    "        width = gray.shape[1]\n",
    "    \n",
    "    # float representation, illuminance in [0,1]\n",
    "    gray_float = gray.astype(float) / 255.0\n",
    "    \n",
    "    # get gradients:\n",
    "    I_x = gradient_x(gray_float)\n",
    "    I_y = gradient_y(gray_float)\n",
    "    \n",
    "    # Ixx, Iyy, Ixy:\n",
    "    Ixx = I_x**2\n",
    "    Ixy = I_y*I_x\n",
    "    Iyy = I_y**2\n",
    "    \n",
    "    # determinant\n",
    "    detA = Ixx * Iyy - Ixy ** 2\n",
    "    \n",
    "    # trace\n",
    "    traceA = Ixx + Iyy\n",
    "    \n",
    "    # Harris response value:\n",
    "    Harris = detA - k * traceA ** 2\n",
    "\n",
    "    return [Harris, Ixx, Iyy, Ixy, BGR, detA, traceA, I_x, I_y]\n",
    "\n",
    "[Harris, Ixx, Iyy, Ixy, BGR, detA, traceA, I_x, I_y] = get_Harris_vectorized('bebop.png')\n",
    "\n",
    "#  plot matrices  for inspection and debugging:\n",
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(BGR, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Image')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(detA)\n",
    "plt.colorbar()\n",
    "plt.title('Determinant')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(Harris)\n",
    "plt.colorbar()\n",
    "plt.title('Harris response')\n",
    "\n",
    "plt.figure()\n",
    "Corners = non_maximum_suppression(Harris)\n",
    "Corners = scipy.ndimage.binary_dilation(Corners)\n",
    "BGR_corners = BGR.copy()\n",
    "BGR_corners[Corners > 0] = [0,0,255]\n",
    "BGR_corners[Harris < 0] = [255,0,0]\n",
    "plt.imshow(cv2.cvtColor(BGR_corners, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Corners in red, lines in blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. First fix and run the script above and run it (or run the fixed script under 'Answers'). Then run the script below. The function `plot_eigenvalues` calculates and plots the two eigenvalues $\\lambda_1$ and $\\lambda_2$ per pixel, showing the image patch at that coordinate. Why did Harris and Stephens not calculate the eigenvalues directly instead of using the function based on the trace and determinant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def plot_eigenvalues(Harris, Ixx, Iyy, Ixy, BGR, k=0.04, IMAGE_PATCHES=False):\n",
    "\n",
    "    height = BGR.shape[0]\n",
    "    width = BGR.shape[1]\n",
    "    \n",
    "    threshold = 5E3 / (height* width)\n",
    "    print(f'Threshold = {threshold}')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    alpha = np.arange(0, 7, 0.01)\n",
    "    beta = np.arange(0, 7, 0.01)\n",
    "    [X,Y] = np.meshgrid(alpha, beta)\n",
    "    R = X * Y - k * (X+Y)**2\n",
    "    levels = np.arange(-3, 6, 1)\n",
    "    cs = plt.contour(X, Y, R, levels)\n",
    "    plt.clabel(cs, levels)\n",
    "    \n",
    "    N = height * width\n",
    "    N_10 = np.ceil(N / 10)\n",
    "    i = 0\n",
    "    \n",
    "    for y in range(1, height - 1):\n",
    "        for x in range(1, width - 1):\n",
    "            #print(f'{x}, {y}')\n",
    "            \n",
    "            i += 1\n",
    "            if(i % N_10 == 0):\n",
    "                p = np.floor(i / N_10) * 10\n",
    "                print(f'{p} procent')\n",
    "            \n",
    "            dx2 = Ixx[y,x]\n",
    "            dy2 = Iyy[y,x]\n",
    "            dxdy = Ixy[y,x]\n",
    "            \n",
    "            M = np.asarray([[dx2, dxdy],[dxdy, dy2]])\n",
    "            ev = np.linalg.eigvals(M)\n",
    "            alpha = ev[0]\n",
    "            beta = ev[1]\n",
    "            \n",
    "            det = dx2 * dy2 - dxdy * dxdy\n",
    "#            if(detA[y,x] != 0):\n",
    "#                print(f'detA[{y},{x}] = {detA[y,x]}, det = {det}')\n",
    "            \n",
    "            r = np.random.rand(1)\n",
    "            if(det != 0 and r < threshold):\n",
    "                plt.plot(alpha, beta, 'kx')\n",
    "                \n",
    "                if(IMAGE_PATCHES):\n",
    "                    Patch = BGR[y-1:y+2, x-1:x+2]\n",
    "                    RGB = deepcopy(Patch)\n",
    "                    RGB[:,:,0] = Patch[:,:,2]\n",
    "                    RGB[:,:,2] = Patch[:,:,0]\n",
    "                    patch = OffsetImage(RGB, zoom=3)  \n",
    "                    ab = AnnotationBbox(patch, (alpha, beta), frameon=False)\n",
    "                    ax.add_artist(ab)\n",
    "    plt.xlabel('$\\lambda_1$')\n",
    "    plt.ylabel('$\\lambda_2$')\n",
    "    plt.title('Eigenvalues')\n",
    "    print(f'{100.0} procent')\n",
    "\n",
    "plot_eigenvalues(Harris, Ixx, Iyy, Ixy, BGR, k=0.04, IMAGE_PATCHES=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try out different values of $k$ (default $0.04$) and run the script below. What is the effect of a higher $k$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Harris_function(k=0.04):\n",
    "    \n",
    "    plt.figure()\n",
    "    alpha = np.arange(0, 7, 0.01)\n",
    "    beta = np.arange(0, 7, 0.01)\n",
    "    [X,Y] = np.meshgrid(alpha, beta)\n",
    "    R = X * Y - k * (X+Y)**2\n",
    "    levels = np.arange(-3, 6, 1)\n",
    "    cs = plt.contour(X, Y, R, levels)\n",
    "    plt.clabel(cs, levels)\n",
    "    plt.xlabel('$\\lambda_1$')\n",
    "    plt.ylabel('$\\lambda_2$')\n",
    "    \n",
    "plot_Harris_function(k=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Lucas, B. and Kanade, T. **An iterative image registration technique with an application to stereo vision.** In proceedings of Imaging understanding workshop, pp. 121–130. 1981.\n",
    "\n",
    "[2] Harris, C., & Stephens, M. (1988, August). **A combined corner and edge detector.** In Alvey vision conference (Vol. 15, p. 50).\n",
    "\n",
    "[3] Moravec, H, **Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover**, Tech Report CMU-RI-TR-3, Carnegie-Mellon University, Robotics Institute, September 1980\n",
    "\n",
    "[4] Muthukrishnan, https://muthu.co/harris-corner-detector-implementation-in-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "<font color='red'><B>Exercise 1.</B></font>\n",
    "\n",
    "1. The worst outcome happens at $x= 0$, where $f'(x) = 0$. If the actual gradient $\\frac{\\delta f}{ \\delta x}$ were used, it would lead to a division by 0. Also, when $f$ and $g$ intersect, the shift will be estimated as $0$.\n",
    "\n",
    "2. When selecting a linear function with a 0 slope, $f'(x) = 0$ and the estimate is always wrong (actually leads to a division by zero in the current code). This corresponds to a uniform texture in the image (e.g., part of a white wall). When selecting the `sine_slope` function with 0 slope, shifts bigger than half the period will lead to wrong estimates. This corresponds to repetitive texture in images. Even when adding a slope, like of 0.5, the method will start to estimate the wrong direction when surpassing the threshold, due to the local minima in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def fun(x, offset=10, slope = -0.5, a = 1, fun_type = 'sine_slope'):    \n",
    "    \n",
    "    if(fun_type == 'sine_slope'):\n",
    "        y = offset + slope * x + np.sin(a * x)\n",
    "    elif(fun_type == 'quadratic'):\n",
    "        y = offset + slope * x + a * x**2\n",
    "    elif(fun_type == 'linear'):\n",
    "        y = offset + slope * x\n",
    "        \n",
    "    return y\n",
    "        \n",
    "def fun_dx(x, offset=10, slope = -0.5, a = 1, fun_type = 'sine_slope'):    \n",
    "    \n",
    "    if(fun_type == 'sine_slope'):\n",
    "        dy_dx = slope + a * np.cos(a * x)\n",
    "    elif(fun_type == 'quadratic'):\n",
    "        dy_dx = slope + 2* a * x\n",
    "    elif(fun_type == 'linear'):\n",
    "        dy_dx = slope\n",
    "    \n",
    "    return dy_dx\n",
    "    \n",
    "def fun_dx2(x, offset=10, slope = -0.5, a = 1, fun_type = 'sine_slope'):    \n",
    "    \n",
    "    if(fun_type == 'sine_slope'):\n",
    "         dy_dxdx = -a*a * np.sin(a * x)\n",
    "    elif(fun_type == 'quadratic'):\n",
    "         dy_dxdx = 2 * a\n",
    "    elif(fun_type == 'linear'):\n",
    "        dy_dxdx = 0\n",
    "        \n",
    "    return dy_dxdx\n",
    "\n",
    "# properties of the function\n",
    "\n",
    "#fun_type = 'quadratic'\n",
    "#offset= 10\n",
    "#slope = 0\n",
    "#a = 2\n",
    "#dx = 0.5\n",
    "\n",
    "#fun_type = 'linear'\n",
    "#offset= 0\n",
    "#slope = 0\n",
    "#a = 0\n",
    "#dx = 0.5\n",
    "\n",
    "fun_type = 'sine_slope'\n",
    "offset= 5\n",
    "slope = 0 #0.5\n",
    "a = 5\n",
    "# ground-truth shift:\n",
    "dx = 2*np.pi/(a*2) # + 0.1\n",
    "\n",
    "# range in which we show the function:\n",
    "max_x = 3\n",
    "step = 0.01 \n",
    "\n",
    "# We take a simple 1D function and shift it in the x-axis:\n",
    "x = np.arange(-max_x, max_x, step)\n",
    "f = fun(x, offset, slope, a, fun_type)\n",
    "g = fun(x+dx, offset, slope, a, fun_type)\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure()\n",
    "plt.plot(x, f, x, g)\n",
    "plt.xticks(np.arange(-max_x, max_x,1))\n",
    "plt.grid()\n",
    "plt.legend(['f', 'g'])\n",
    "plt.show()\n",
    "\n",
    "# x coordinate at which we apply Eq. 4:\n",
    "x_query = 0\n",
    "\n",
    "# Determine f'(x) and apply Eq. 4:\n",
    "small = 0.01\n",
    "df_dx = (fun(x_query+small, offset, slope, a, fun_type) - fun(x_query, offset, slope, a, fun_type)) / small\n",
    "h = (fun(x_query+dx, offset, slope, a, fun_type) - fun(x_query, offset, slope, a, fun_type)) / df_dx\n",
    "\n",
    "# Output result:\n",
    "print(f'Lucas and Kanade say that the shift at {x_query} = {h}, while the ground truth is {dx}')\n",
    "print(f'Approximation df/dx = {df_dx}, real value = {fun_dx(x_query, offset, slope, a, fun_type)}')\n",
    "print(f'df/dx({x_query}) = {fun_dx(x_query, offset, slope, a, fun_type)}, df/dx2({x_query}) = {fun_dx2(x_query, offset, slope, a, fun_type)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><B>Exercise 2.</B></font>\n",
    "\n",
    "1. In most cases the window works well, but even for the simple quadratic function, it still works rather badly when $f'(x) = 0$.\n",
    "\n",
    "<font color='red'><B>Exercise 3.</B></font>\n",
    "\n",
    "1. Yes, it is solved. Pixels with $f'(x)=0$ are not taken into account in the new weighing. A division by zero could still happen, but only if there is no gradient in any of the pixels. In that case, the input is completely constant and flow cannot be determined.\n",
    "\n",
    "<font color='red'><B>Exercise 4.</B></font>\n",
    "1. The error in the code lies in the determination of Ixx, Ixy, Iyy: they need to be convolved with a Gaussian kernel. Without this convolution, the determinant is always zero: $\\mathrm{det(M)=I_x I_x I_y I_y - I_x I_y I_x I_y = 0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from scipy import signal\n",
    "\n",
    "def gradient_x(imggray):\n",
    "    # Sobel kernel for Ix.\n",
    "    kernel_x = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\n",
    "    return signal.convolve2d(imggray, kernel_x, mode='same')\n",
    "\n",
    "def gradient_y(imggray):\n",
    "    # Sobel kernel for Iy:\n",
    "    kernel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    return signal.convolve2d(imggray, kernel_y, mode='same')\n",
    "\n",
    "def non_maximum_suppression(Harris, dist = 1, threshold_factor = 0.05):\n",
    "    # suppress non maxima:\n",
    "    max_H = np.max(Harris[:])\n",
    "    threshold = threshold_factor * max_H\n",
    "    \n",
    "    Corners = np.zeros([Harris.shape[0], Harris.shape[1]])\n",
    "    for x in range(1, Harris.shape[1]-1):\n",
    "        for y in range(1, Harris.shape[0]-1):\n",
    "            if Harris[y,x] >= threshold:\n",
    "                Harris[y-dist:y+dist+1, x-dist:x+dist+1] = 0 # suppress neighbors\n",
    "                Corners[y,x] = 1 # indicate corner\n",
    "    return Corners\n",
    "\n",
    "def get_Harris_vectorized(filename, k = 0.04):\n",
    "    # load the BGR color image:\n",
    "    BGR = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(BGR, cv2.COLOR_BGR2GRAY)\n",
    "    height = gray.shape[0]\n",
    "    width = gray.shape[1]\n",
    "    \n",
    "    # resize if too big:\n",
    "    if(height * width >= 1E5):\n",
    "        gray = cv2.resize(gray, (int(width/4), int(height/4)))\n",
    "        BGR = cv2.resize(BGR, (int(width/4), int(height/4)))\n",
    "        height = gray.shape[0]\n",
    "        width = gray.shape[1]\n",
    "    \n",
    "    # float representation, illuminance in [0,1]\n",
    "    gray_float = gray.astype(float) / 255.0\n",
    "    \n",
    "    # get gradients:\n",
    "    I_x = gradient_x(gray_float)\n",
    "    I_y = gradient_y(gray_float)\n",
    "    \n",
    "    # Ixx, Iyy, Ixy:\n",
    "    Ixx = scipy.ndimage.gaussian_filter(I_x**2, sigma=1)\n",
    "    Ixy = scipy.ndimage.gaussian_filter(I_y*I_x, sigma=1)\n",
    "    Iyy = scipy.ndimage.gaussian_filter(I_y**2, sigma=1)\n",
    "    \n",
    "    # determinant\n",
    "    detA = Ixx * Iyy - Ixy ** 2\n",
    "    \n",
    "    # trace\n",
    "    traceA = Ixx + Iyy\n",
    "    \n",
    "    # Harris response value:\n",
    "    Harris = detA - k * traceA ** 2\n",
    "\n",
    "    return [Harris, Ixx, Iyy, Ixy, BGR, detA, traceA, I_x, I_y]\n",
    "\n",
    "[Harris, Ixx, Iyy, Ixy, BGR, detA, traceA, I_x, I_y] = get_Harris_vectorized('bebop.png')\n",
    "\n",
    "#  plot matrices  for inspection and debugging:\n",
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(BGR, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Image')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(detA)\n",
    "plt.colorbar()\n",
    "plt.title('Determinant')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(Harris)\n",
    "plt.colorbar()\n",
    "plt.title('Harris response')\n",
    "\n",
    "plt.figure()\n",
    "Corners = non_maximum_suppression(Harris)\n",
    "Corners = scipy.ndimage.binary_dilation(Corners)\n",
    "BGR_corners = BGR.copy()\n",
    "BGR_corners[Corners > 0] = [0,0,255]\n",
    "BGR_corners[Harris < 0] = [255,0,0]\n",
    "plt.imshow(cv2.cvtColor(BGR_corners, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Corners in red, lines in blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. It takes a long time to calculate the eigenvalues for all pixels. The trace and determinant are very easy to calculate. Since they are related to the eigenvalues, i.e.,  $\\mathrm{det}(M) = \\lambda_1 \\lambda_2$ and $\\mathrm{tr}(M) = \\lambda_1 + \\lambda_2$, they still capture the principal curvature of the image patch. \n",
    "\n",
    "3. A higher $k$ leads to a larger region where $R < 0$, so is stricter for the acceptance of a patch as a corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
